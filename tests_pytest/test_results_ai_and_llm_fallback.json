{
  "created": 1750776947.532724,
  "duration": 0.14542436599731445,
  "exitcode": 1,
  "root": "F:\\Asystent",
  "environment": {},
  "summary": { "error": 17, "total": 17, "collected": 18, "deselected": 1 },
  "collectors": [
    {
      "nodeid": "",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py",
          "type": "Module"
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_short_term_memory_persistence",
          "type": "Coroutine",
          "lineno": 20
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_mid_term_memory_daily_reset",
          "type": "Coroutine",
          "lineno": 38
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_long_term_memory_persistence",
          "type": "Coroutine",
          "lineno": 52
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_memory_fallback_handling",
          "type": "Coroutine",
          "lineno": 69
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_habit_recognition",
          "type": "Coroutine",
          "lineno": 85
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_habit_suggestions",
          "type": "Coroutine",
          "lineno": 103
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_behavior_logging",
          "type": "Coroutine",
          "lineno": 114
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_gpt_nano_backend",
          "type": "Coroutine",
          "lineno": 137
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_api_error_handling",
          "type": "Coroutine",
          "lineno": 148
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_fallback_metadata",
          "type": "Coroutine",
          "lineno": 163
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_token_limit_handling",
          "type": "Coroutine",
          "lineno": 177
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_separate_user_sessions",
          "type": "Coroutine",
          "lineno": 194
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_no_data_mixing",
          "type": "Coroutine",
          "lineno": 215
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_multiple_active_users",
          "type": "Coroutine",
          "lineno": 245
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_user_switching_simulation",
          "type": "Coroutine",
          "lineno": 266
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios::test_plugin_memory_immediate_use",
          "type": "Coroutine",
          "lineno": 298
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios::test_plugin_fallback_chain",
          "type": "Coroutine",
          "lineno": 315
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios::test_response_time_consistency",
          "type": "Coroutine",
          "lineno": 326,
          "deselected": true
        }
      ]
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py",
      "outcome": "passed",
      "result": [
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager",
          "type": "Class"
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning",
          "type": "Class"
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback",
          "type": "Class"
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic",
          "type": "Class"
        },
        {
          "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios",
          "type": "Class"
        }
      ]
    }
  ],
  "tests": [
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_short_term_memory_persistence",
      "lineno": 20,
      "outcome": "error",
      "keywords": [
        "test_short_term_memory_persistence",
        "integration",
        "asyncio",
        "pytestmark",
        "TestMemoryManager",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0027552000028663315,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 21\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_short_term_memory_persistence(self, http_session, server_helper, test_user_id):\n          \"\"\"Short-term memory trzyma dane ok. 15\u201320 minut\"\"\"\n          # Store information in short-term memory\n          query1 = \"Zapisz w pami\u0119ci \u017ce lubi\u0119 kaw\u0119\"\n          response1 = await server_helper.make_query_request(http_session, test_user_id, query1)\n          assert \"ai_response\" in response1\n\n          # Immediately ask about stored information\n          query2 = \"Co lubi\u0119 pi\u0107?\"\n          response2 = await server_helper.make_query_request(http_session, test_user_id, query2)\n          assert \"ai_response\" in response2\n\n          # Response should reference the stored information\n          # Note: This requires the server to actually implement memory\n          assert len(response2[\"ai_response\"]) > 0\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:21"
      },
      "teardown": { "duration": 0.00023289999808184803, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_mid_term_memory_daily_reset",
      "lineno": 38,
      "outcome": "error",
      "keywords": [
        "test_mid_term_memory_daily_reset",
        "integration",
        "asyncio",
        "pytestmark",
        "TestMemoryManager",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00039710000055492856,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 39\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_mid_term_memory_daily_reset(self, http_session, server_helper, test_user_id):\n          \"\"\"Mid-term memory trzyma dane dzienne (reset po p\u00f3\u0142nocy lub r\u0119cznie)\"\"\"\n          # Store daily information\n          query = \"Dzisiaj mia\u0142em spotkanie o 14:00\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n          assert \"ai_response\" in response\n\n          # Query about today's events\n          query2 = \"Co robi\u0142em dzisiaj?\"\n          response2 = await server_helper.make_query_request(http_session, test_user_id, query2)\n          assert \"ai_response\" in response2\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:39"
      },
      "teardown": { "duration": 0.00023159999909694307, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_long_term_memory_persistence",
      "lineno": 52,
      "outcome": "error",
      "keywords": [
        "test_long_term_memory_persistence",
        "integration",
        "asyncio",
        "pytestmark",
        "TestMemoryManager",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00034469999809516594,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 53\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_long_term_memory_persistence(self, http_session, server_helper, test_user_id):\n          \"\"\"Long-term memory zapisuje do bazy (SQLite) i odczytuje przy starcie\"\"\"\n          # Store important information for long-term memory\n          query = \"Zapami\u0119taj na d\u0142ugo: moje ulubione miasto to Krak\u00f3w\"\n          response = await server_helper.make_query_request(\n              http_session,\n              test_user_id,\n              query,\n              {\"memory_type\": \"long_term\"}\n          )\n          assert \"ai_response\" in response\n\n          # This would need to be tested across server restarts\n          # For now, just verify the response acknowledges storage\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:53"
      },
      "teardown": { "duration": 0.00021329999799490906, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestMemoryManager::test_memory_fallback_handling",
      "lineno": 69,
      "outcome": "error",
      "keywords": [
        "test_memory_fallback_handling",
        "integration",
        "asyncio",
        "pytestmark",
        "TestMemoryManager",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0003375000014784746,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 70\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_memory_fallback_handling(self, http_session, server_helper, test_user_id):\n          \"\"\"Fallback do \u201ebrak pami\u0119ci\" nie crashuje odpowiedzi\"\"\"\n          # Query about non-existent memory\n          query = \"Co m\u00f3wi\u0142em wczoraj o nieistniej\u0105cym temacie xyz123?\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n\n          assert \"ai_response\" in response\n          assert len(response[\"ai_response\"]) > 0\n          # Should gracefully handle lack of memory\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:70"
      },
      "teardown": { "duration": 0.00021040000137872994, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_habit_recognition",
      "lineno": 85,
      "outcome": "error",
      "keywords": [
        "test_habit_recognition",
        "integration",
        "asyncio",
        "pytestmark",
        "TestHabitLearning",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.000325700002576923,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 86\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_habit_recognition(self, http_session, server_helper, test_user_id):\n          \"\"\"System zapisuje powtarzalne zapytania i pory dnia\"\"\"\n          # Simulate repeating behavior\n          morning_query = \"Jaka jest pogoda?\"\n\n          # Make the same query multiple times to establish pattern\n          for i in range(3):\n              response = await server_helper.make_query_request(\n                  http_session,\n                  test_user_id,\n                  morning_query,\n                  {\"timestamp\": datetime.now().isoformat()}\n              )\n              assert \"ai_response\" in response\n              await asyncio.sleep(0.1)  # Small delay\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:86"
      },
      "teardown": { "duration": 0.00020509999740170315, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_habit_suggestions",
      "lineno": 103,
      "outcome": "error",
      "keywords": [
        "test_habit_suggestions",
        "integration",
        "asyncio",
        "pytestmark",
        "TestHabitLearning",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00033940000139409676,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 104\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_habit_suggestions(self, http_session, server_helper, test_user_id):\n          \"\"\"Potrafi zasugerowa\u0107 automatyczne akcje\"\"\"\n          # After establishing pattern, ask for suggestions\n          query = \"Masz jakie\u015b sugestie dla mnie?\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n\n          assert \"ai_response\" in response\n          # Response should potentially include suggestions based on patterns\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:104"
      },
      "teardown": { "duration": 0.00021969999943394214, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestHabitLearning::test_behavior_logging",
      "lineno": 114,
      "outcome": "error",
      "keywords": [
        "test_behavior_logging",
        "integration",
        "asyncio",
        "pytestmark",
        "TestHabitLearning",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00033149999944726005,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 115\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_behavior_logging(self, http_session, server_helper, test_user_id):\n          \"\"\"Zachowania s\u0105 logowane (czas + tre\u015b\u0107 + intencja)\"\"\"\n          queries = [\n              \"Sprawd\u017a email\",\n              \"Jaka jest pogoda?\",\n              \"Ustaw alarm na 7:00\",\n          ]\n\n          for query in queries:\n              response = await server_helper.make_query_request(\n                  http_session,\n                  test_user_id,\n                  query,\n                  {\"timestamp\": datetime.now().isoformat()}\n              )\n              assert \"ai_response\" in response\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:115"
      },
      "teardown": { "duration": 0.00020940000104019418, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_gpt_nano_backend",
      "lineno": 137,
      "outcome": "error",
      "keywords": [
        "test_gpt_nano_backend",
        "integration",
        "asyncio",
        "pytestmark",
        "TestAIAndLLMFallback",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0003102000009675976,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 138\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_gpt_nano_backend(self, http_session, server_helper, test_user_id):\n          \"\"\"Dzia\u0142a gpt-4.1-nano jako domy\u015blny backend\"\"\"\n          # Query that should trigger LLM\n          query = \"Opowiedz mi kr\u00f3tk\u0105 histori\u0119 o robotach\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n\n          assert \"ai_response\" in response\n          assert len(response[\"ai_response\"]) > 50  # Should be a decent response\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:138"
      },
      "teardown": { "duration": 0.00021010000273236074, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_api_error_handling",
      "lineno": 148,
      "outcome": "error",
      "keywords": [
        "test_api_error_handling",
        "integration",
        "asyncio",
        "pytestmark",
        "TestAIAndLLMFallback",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00038380000114557333,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 149\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_api_error_handling(self, http_session, server_helper, test_user_id):\n          \"\"\"Obs\u0142uga b\u0142\u0119d\u00f3w API (rate limit, 401, brak po\u0142\u0105czenia)\"\"\"\n          # Test with potentially problematic query\n          query = \"Generate a very long response\" * 10  # Might hit limits\n\n          try:\n              response = await server_helper.make_query_request(http_session, test_user_id, query)\n              # Should either succeed or handle gracefully\n              assert \"ai_response\" in response\n          except Exception:\n              # Server should handle API errors gracefully\n              pass\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:149"
      },
      "teardown": { "duration": 0.00024119999943650328, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_fallback_metadata",
      "lineno": 163,
      "outcome": "error",
      "keywords": [
        "test_fallback_metadata",
        "integration",
        "asyncio",
        "pytestmark",
        "TestAIAndLLMFallback",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00036869999894406646,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 164\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_fallback_metadata(self, http_session, server_helper, test_user_id):\n          \"\"\"Odpowied\u017a fallbacka zawiera meta-info (\u017ce to fallback)\"\"\"\n          # Query that likely uses fallback\n          query = \"Explain quantum physics in simple terms\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n\n          assert \"ai_response\" in response\n          # Check if metadata indicates fallback usage\n          if \"metadata\" in response or \"source\" in response:\n              # Metadata should indicate LLM/fallback usage\n              pass\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:164"
      },
      "teardown": { "duration": 0.0002122999976563733, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestAIAndLLMFallback::test_token_limit_handling",
      "lineno": 177,
      "outcome": "error",
      "keywords": [
        "test_token_limit_handling",
        "integration",
        "asyncio",
        "pytestmark",
        "TestAIAndLLMFallback",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0003318999988550786,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 178\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_token_limit_handling(self, http_session, server_helper, test_user_id):\n          \"\"\"Token limit i retry policy dzia\u0142aj\u0105 poprawnie\"\"\"\n          # Very long query that might hit token limits\n          long_query = \"Explain \" + \"very detailed \" * 100 + \"machine learning concepts\"\n\n          response = await server_helper.make_query_request(http_session, test_user_id, long_query)\n\n          assert \"ai_response\" in response\n          # Should handle long queries without crashing\n          assert len(response[\"ai_response\"]) > 0\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:178"
      },
      "teardown": { "duration": 0.0002136999974027276, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_separate_user_sessions",
      "lineno": 194,
      "outcome": "error",
      "keywords": [
        "test_separate_user_sessions",
        "integration",
        "asyncio",
        "pytestmark",
        "TestSessionAndUserLogic",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0003729999989445787,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 195\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_separate_user_sessions(self, http_session, server_helper):\n          \"\"\"Ka\u017cdy u\u017cytkownik ma odr\u0119bn\u0105 sesj\u0119 (UUID / token)\"\"\"\n          user1_id = \"session_test_user_1\"\n          user2_id = \"session_test_user_2\"\n\n          # User 1 stores information\n          response1 = await server_helper.make_query_request(\n              http_session, user1_id, \"Zapami\u0119taj \u017ce lubi\u0119 pizz\u0119\"\n          )\n          assert \"ai_response\" in response1\n\n          # User 2 asks about user 1's information - should not have access\n          response2 = await server_helper.make_query_request(\n              http_session, user2_id, \"Co lubi user1?\"\n          )\n          assert \"ai_response\" in response2\n\n          # User 2 should not know about user 1's preferences\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:195"
      },
      "teardown": { "duration": 0.00022700000045006163, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_no_data_mixing",
      "lineno": 215,
      "outcome": "error",
      "keywords": [
        "test_no_data_mixing",
        "integration",
        "asyncio",
        "pytestmark",
        "TestSessionAndUserLogic",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00031970000054570846,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 216\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_no_data_mixing(self, http_session, server_helper):\n          \"\"\"Dane nie mieszaj\u0105 si\u0119 mi\u0119dzy u\u017cytkownikami\"\"\"\n          user1_id = \"data_isolation_user_1\"\n          user2_id = \"data_isolation_user_2\"\n\n          # Both users store different information\n          await server_helper.make_query_request(\n              http_session, user1_id, \"Moja ulubiona liczba to 42\"\n          )\n\n          await server_helper.make_query_request(\n              http_session, user2_id, \"Moja ulubiona liczba to 100\"\n          )\n\n          # Each user should get their own information back\n          response1 = await server_helper.make_query_request(\n              http_session, user1_id, \"Jaka jest moja ulubiona liczba?\"\n          )\n\n          response2 = await server_helper.make_query_request(\n              http_session, user2_id, \"Jaka jest moja ulubiona liczba?\"\n          )\n\n          assert \"ai_response\" in response1\n          assert \"ai_response\" in response2\n\n          # Responses should be different (or at least isolated)\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:216"
      },
      "teardown": { "duration": 0.0002082999999402091, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_multiple_active_users",
      "lineno": 245,
      "outcome": "error",
      "keywords": [
        "test_multiple_active_users",
        "integration",
        "asyncio",
        "pytestmark",
        "TestSessionAndUserLogic",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.0003136000013910234,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 246\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_multiple_active_users(self, http_session, server_helper):\n          \"\"\"Serwer potrafi trzyma\u0107 kilka aktywnych u\u017cytkownik\u00f3w naraz\"\"\"\n          users = [f\"concurrent_user_{i}\" for i in range(5)]\n\n          # All users make requests simultaneously\n          tasks = []\n          for user_id in users:\n              task = server_helper.make_query_request(\n                  http_session, user_id, f\"Hello from {user_id}\"\n              )\n              tasks.append(task)\n\n          responses = await asyncio.gather(*tasks)\n\n          # All requests should succeed\n          assert len(responses) == 5\n          for response in responses:\n              assert \"ai_response\" in response\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:246"
      },
      "teardown": { "duration": 0.00021879999985685572, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestSessionAndUserLogic::test_user_switching_simulation",
      "lineno": 266,
      "outcome": "error",
      "keywords": [
        "test_user_switching_simulation",
        "integration",
        "asyncio",
        "pytestmark",
        "TestSessionAndUserLogic",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00039789999937056564,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 267\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_user_switching_simulation(self, http_session, server_helper):\n          \"\"\"Mo\u017cna prze\u0142\u0105cza\u0107 u\u017cytkownika (symulacja z klienta)\"\"\"\n          # Simulate client switching between users\n          user1_id = \"switch_test_user_1\"\n          user2_id = \"switch_test_user_2\"\n\n          # Request as user 1\n          response1 = await server_helper.make_query_request(\n              http_session, user1_id, \"Test jako user 1\"\n          )\n          assert \"ai_response\" in response1\n\n          # Switch to user 2\n          response2 = await server_helper.make_query_request(\n              http_session, user2_id, \"Test jako user 2\"\n          )\n          assert \"ai_response\" in response2\n\n          # Switch back to user 1\n          response3 = await server_helper.make_query_request(\n              http_session, user1_id, \"Znowu jako user 1\"\n          )\n          assert \"ai_response\" in response3\n\n          # All should work independently\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:267"
      },
      "teardown": { "duration": 0.0002367999986745417, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios::test_plugin_memory_immediate_use",
      "lineno": 298,
      "outcome": "error",
      "keywords": [
        "test_plugin_memory_immediate_use",
        "slow",
        "integration",
        "asyncio",
        "pytestmark",
        "TestExtendedScenarios",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00031149999995250255,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 299\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      @pytest.mark.slow\n      async def test_plugin_memory_immediate_use(self, http_session, server_helper, test_user_id):\n          \"\"\"Odpowied\u017a z pluginu + zapis wspomnienia + natychmiastowe u\u017cycie tej wiedzy\"\"\"\n          # Get weather (plugin response) and store it\n          response1 = await server_helper.make_query_request(\n              http_session, test_user_id, \"Jaka jest pogoda? Zapami\u0119taj odpowied\u017a.\"\n          )\n          assert \"ai_response\" in response1\n\n          # Immediately ask about stored weather information\n          response2 = await server_helper.make_query_request(\n              http_session, test_user_id, \"Jak\u0105 pogod\u0119 sprawdza\u0142em przed chwil\u0105?\"\n          )\n          assert \"ai_response\" in response2\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:299"
      },
      "teardown": { "duration": 0.00020679999943240546, "outcome": "passed" }
    },
    {
      "nodeid": "tests_pytest/test_server_memory_sessions.py::TestExtendedScenarios::test_plugin_fallback_chain",
      "lineno": 315,
      "outcome": "error",
      "keywords": [
        "test_plugin_fallback_chain",
        "integration",
        "asyncio",
        "pytestmark",
        "TestExtendedScenarios",
        "test_server_memory_sessions.py",
        "tests_pytest",
        "Asystent",
        ""
      ],
      "setup": {
        "duration": 0.00033489999987068586,
        "outcome": "failed",
        "longrepr": "file F:\\Asystent\\tests_pytest\\test_server_memory_sessions.py, line 316\n      @pytest.mark.asyncio\n      @pytest.mark.integration\n      async def test_plugin_fallback_chain(self, http_session, server_helper, test_user_id):\n          \"\"\"Brak odpowiedzi z pluginu \u2192 fallback do LLM\"\"\"\n          # Query that might fail plugin but should get LLM response\n          query = \"Explain the philosophical implications of artificial intelligence\"\n          response = await server_helper.make_query_request(http_session, test_user_id, query)\n\n          assert \"ai_response\" in response\n          assert len(response[\"ai_response\"]) > 20  # Should get substantial LLM response\nE       fixture 'http_session' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, _session_faker, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, client_app, cov, doctest_namespace, event_loop, event_loop_policy, extra, extras, faker, include_metadata_in_junit_xml, json_metadata, metadata, mock_ai_module, mock_audio_components, mock_config, mock_db_manager, mock_plugin, mock_plugin_manager, mock_websocket, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_chat_data, sample_plugin_data, server_app, session_mocker, temp_config_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nF:\\Asystent\\tests_pytest\\test_server_memory_sessions.py:316"
      },
      "teardown": { "duration": 0.00032669999927747995, "outcome": "passed" }
    }
  ]
}
