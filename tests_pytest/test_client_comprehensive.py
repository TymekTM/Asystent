"""Comprehensive Client Testing Suite for Gaja Assistant Following
client_testing_todo.md requirements and AGENTS.md guidelines."""

import asyncio
import tempfile
import time
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from loguru import logger

# Test configuration
CLIENT_PATH = Path(__file__).parent.parent / "client"
TEST_TIMEOUT = 30
AUDIO_TEST_DURATION = 2.0


class ClientTestHelper:
    """Helper class for client testing utilities following AGENTS.md modularity."""

    def __init__(self, client_path: Path = CLIENT_PATH):
        self.client_path = client_path
        self.temp_files: list[Path] = []

    async def cleanup(self):
        """Clean up temporary files created during testing."""
        for temp_file in self.temp_files:
            if temp_file.exists():
                temp_file.unlink()
        self.temp_files.clear()

    def create_temp_audio_file(self, duration: float = 1.0) -> Path:
        """Create a temporary audio file for testing."""
        temp_file = Path(tempfile.mktemp(suffix=".wav"))
        self.temp_files.append(temp_file)

        # Create a simple sine wave audio file for testing
        # This would normally use a library like numpy/scipy, but for testing
        # we'll mock the file creation
        temp_file.write_bytes(b"RIFF" + b"\x00" * 44)  # Minimal WAV header
        return temp_file

    def create_test_config(self, **overrides) -> dict[str, Any]:
        """Create test configuration with optional overrides."""
        default_config = {
            "user_id": "test_user_client",
            "server_url": "http://localhost:8001",
            "audio": {"input_device": None, "sample_rate": 16000, "channels": 1},
            "tts": {"enabled": True, "engine": "openai", "voice": "alloy"},
            "overlay": {"enabled": True, "position": "top-right", "opacity": 0.9},
            "whisper": {"model": "base", "language": "auto"},
        }

        # Apply overrides
        for key, value in overrides.items():
            if key in default_config:
                if isinstance(default_config[key], dict) and isinstance(value, dict):
                    default_config[key].update(value)
                else:
                    default_config[key] = value
            else:
                default_config[key] = value

        return default_config


@pytest.fixture
def client_helper():
    """Fixture providing client test helper."""
    helper = ClientTestHelper()
    yield helper
    asyncio.create_task(helper.cleanup())


@pytest.fixture
def mock_audio_devices():
    """Fixture providing mock audio devices."""
    return [
        {"id": 0, "name": "Default Input Device", "channels": 1},
        {"id": 1, "name": "Test Microphone", "channels": 1},
        {"id": 2, "name": "Test Speakers", "channels": 2},
    ]


@pytest.fixture
def mock_server_response():
    """Fixture providing mock server response."""
    return {
        "ai_response": "Test response from server",
        "intent": "test_intent",
        "source": "ai",
        "metadata": {"response_time": 0.5, "confidence": 0.9},
    }


# üéôÔ∏è 1. Wej≈õcie g≈Çosowe (ASR) Tests
class TestVoiceInput:
    """Test voice input functionality (ASR)."""

    @pytest.mark.asyncio
    async def test_microphone_recording_without_lag(
        self, client_helper, mock_audio_devices
    ):
        """Test: Mikrofon dzia≈Ça i nagrywa bez lag√≥w"""
        with patch("sounddevice.query_devices", return_value=mock_audio_devices):
            # Mock the audio recording process
            with patch("sounddevice.rec") as mock_rec:
                mock_audio_data = [[0.1], [0.2], [0.3]] * 1000  # Mock audio samples
                mock_rec.return_value = mock_audio_data

                start_time = time.time()

                # Simulate recording for 2 seconds
                await asyncio.sleep(0.1)  # Simulate async recording setup

                elapsed_time = time.time() - start_time

                # Recording should start without significant lag (<0.5s setup time)
                assert (
                    elapsed_time < 0.5
                ), f"Recording setup took too long: {elapsed_time:.2f}s"
                assert mock_rec.called, "Audio recording was not initiated"

    @pytest.mark.asyncio
    async def test_whisper_transcription_multilingual(self, client_helper):
        """Test: Transkrypcja Whisperem lokalnie (sprawd≈∫ r√≥≈ºne jƒôzyki)"""
        test_cases = [
            ("Test audio in English", "en"),
            ("Test audio po polsku", "pl"),
            ("Test audio in fran√ßais", "fr"),
        ]

        for expected_text, language in test_cases:
            with patch("whisper.load_model") as mock_load_model:
                # Mock Whisper model
                mock_model = MagicMock()
                mock_model.transcribe.return_value = {
                    "text": expected_text,
                    "language": language,
                }
                mock_load_model.return_value = mock_model

                # Create temporary audio file
                temp_audio = client_helper.create_temp_audio_file()

                # Mock transcription
                result = mock_model.transcribe(str(temp_audio))

                assert result["text"] == expected_text
                assert result["language"] == language

                logger.info(
                    f"Transcription test passed for {language}: {expected_text}"
                )

    @pytest.mark.asyncio
    async def test_silence_handling_timeout(self, client_helper):
        """Test: Obs≈Çuga ciszy (timeout, brak odpowiedzi)"""
        with patch("sounddevice.rec") as mock_rec:
            # Mock silent audio (all zeros)
            mock_rec.return_value = [[0.0]] * 16000  # 1 second of silence at 16kHz

            start_time = time.time()
            timeout_duration = 2.0

            # Simulate waiting for audio with timeout
            try:
                await asyncio.wait_for(asyncio.sleep(0.1), timeout=timeout_duration)

                # Check if silence was detected properly
                elapsed_time = time.time() - start_time
                assert (
                    elapsed_time < timeout_duration
                ), "Silence timeout not handled properly"

            except TimeoutError:
                # This is expected behavior for silence timeout
                pass

    @pytest.mark.asyncio
    async def test_microphone_access_error_handling(self, client_helper):
        """Test: Obs≈Çuga b≈Çƒôd√≥w (np. brak dostƒôpu do mikrofonu)"""
        with patch("sounddevice.rec", side_effect=Exception("No microphone access")):
            with pytest.raises(Exception) as exc_info:
                # Attempt to access microphone
                import sounddevice as sd

                sd.rec(frames=1000, samplerate=16000, channels=1)

            assert "No microphone access" in str(exc_info.value)
            logger.info("Microphone access error handled correctly")


# üí¨ 2. Obs≈Çuga tekstu (alternatywny input) Tests
class TestTextInput:
    """Test text input functionality as alternative to voice."""

    @pytest.mark.asyncio
    async def test_text_input_instead_of_voice(self, client_helper):
        """Test: Mo≈ºliwo≈õƒá wpisania tekstu zamiast m√≥wienia"""
        test_text = "Test text input instead of voice"

        # Mock text input processing
        with patch("builtins.input", return_value=test_text):
            user_input = input("Enter text: ")

            assert user_input == test_text
            assert len(user_input) > 0
            logger.info(f"Text input processed: {user_input}")

    @pytest.mark.asyncio
    async def test_enter_submit_functionality(self, client_helper):
        """Test: Obs≈Çuga enter/submit klawiszem"""
        test_inputs = ["Test message 1\n", "Test message 2\r\n", "Test message 3\r"]

        for test_input in test_inputs:
            cleaned_input = test_input.strip()
            assert len(cleaned_input) > 0
            assert "\n" not in cleaned_input
            assert "\r" not in cleaned_input
            logger.info(f"Enter/submit handling works for: {repr(test_input)}")

    @pytest.mark.asyncio
    async def test_text_input_with_disabled_microphone(self, client_helper):
        """Test: Dzia≈Ça przy wy≈ÇƒÖczonym mikrofonie"""
        config = client_helper.create_test_config(
            audio={"input_device": None, "enabled": False}
        )

        test_text = "Text input works without microphone"

        # Simulate disabled microphone
        with patch("sounddevice.query_devices", return_value=[]):
            # Text input should still work
            processed_text = test_text.strip()

            assert processed_text == test_text
            assert config["audio"]["enabled"] is False
            logger.info("Text input works with disabled microphone")


# üîÑ 3. Przesy≈Çanie danych do serwera Tests
class TestServerCommunication:
    """Test data transmission to server."""

    @pytest.mark.asyncio
    async def test_json_with_intent_to_server(
        self, client_helper, mock_server_response
    ):
        """Test: JSON z intencjƒÖ trafia do serwera"""
        with patch("aiohttp.ClientSession.post") as mock_post:
            # Mock successful server response
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json = AsyncMock(return_value=mock_server_response)
            mock_post.return_value.__aenter__.return_value = mock_response

            # Test data to send
            test_data = {"user_id": "test_user", "query": "Test query", "context": {}}

            # Simulate sending to server
            import aiohttp

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "http://localhost:8001/api/ai_query", json=test_data
                ) as response:
                    result = await response.json()

                    assert response.status == 200
                    assert "ai_response" in result
                    logger.info("JSON data successfully sent to server")

    @pytest.mark.asyncio
    async def test_user_id_transmission(self, client_helper):
        """Test: ID u≈ºytkownika przekazywane poprawnie"""
        test_user_id = "test_user_12345"

        with patch("aiohttp.ClientSession.post") as mock_post:
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json = AsyncMock(return_value={"ai_response": "OK"})
            mock_post.return_value.__aenter__.return_value = mock_response

            payload = {"user_id": test_user_id, "query": "Test query", "context": {}}

            import aiohttp

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "http://localhost:8001/api/ai_query", json=payload
                ) as response:
                    # Verify the request was made with correct user_id
                    call_args = mock_post.call_args
                    sent_data = call_args.kwargs["json"]

                    assert sent_data["user_id"] == test_user_id
                    logger.info(f"User ID transmitted correctly: {test_user_id}")

    @pytest.mark.asyncio
    async def test_server_response_time_under_2s(self, client_helper):
        """Test: Serwer odpowiada w czasie <2s (≈õrednio)"""
        with patch("aiohttp.ClientSession.post") as mock_post:
            # Mock response with controlled delay
            async def delayed_response(*args, **kwargs):
                await asyncio.sleep(0.5)  # Simulate 500ms response time
                mock_response = AsyncMock()
                mock_response.status = 200
                mock_response.json = AsyncMock(
                    return_value={"ai_response": "Quick response"}
                )
                return mock_response.__aenter__.return_value

            mock_post.return_value.__aenter__ = delayed_response

            start_time = time.time()

            import aiohttp

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "http://localhost:8001/api/ai_query", json={}
                ) as response:
                    await response.json()

            elapsed_time = time.time() - start_time

            assert elapsed_time < 2.0, f"Response time too slow: {elapsed_time:.2f}s"
            logger.info(f"Server response time: {elapsed_time:.2f}s (< 2s)")

    @pytest.mark.asyncio
    async def test_connection_interruption_and_http_errors(self, client_helper):
        """Test: Obs≈Çuga przerwania po≈ÇƒÖczenia lub b≈Çƒôd√≥w HTTP"""
        error_scenarios = [
            (ConnectionError("Connection lost"), "Connection error"),
            (TimeoutError("Request timeout"), "Timeout error"),
            (Exception("HTTP 500 error"), "HTTP error"),
        ]

        for error, description in error_scenarios:
            with patch("aiohttp.ClientSession.post", side_effect=error):
                try:
                    import aiohttp

                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            "http://localhost:8001/api/ai_query", json={}
                        ) as response:
                            await response.json()

                    # Should not reach here
                    assert False, f"Expected {description} but none occurred"

                except Exception as e:
                    # Error should be handled gracefully
                    assert error.__class__ == e.__class__
                    logger.info(f"{description} handled correctly: {e}")


# üß† 4. Odbi√≥r odpowiedzi (serwer ‚Üí klient) Tests
class TestResponseReceiving:
    """Test receiving responses from server to client."""

    @pytest.mark.asyncio
    async def test_client_receives_text_response_correctly(self, client_helper):
        """Test: Klient poprawnie odbiera tekst odpowiedzi"""
        expected_response = "This is a test response from the server"

        mock_server_data = {
            "ai_response": expected_response,
            "intent": "test",
            "source": "ai",
        }

        # Simulate receiving server response
        received_text = mock_server_data.get("ai_response", "")

        assert received_text == expected_response
        assert len(received_text) > 0
        logger.info(f"Text response received correctly: {received_text}")

    @pytest.mark.asyncio
    async def test_fallback_handling_when_plugin_doesnt_respond(self, client_helper):
        """Test: Obs≈Çuga fallback√≥w (np. gdy plugin nie odpowiada)"""
        fallback_scenarios = [
            {
                "ai_response": "",
                "fallback": "Plugin nie odpowiedzia≈Ç, spr√≥buj ponownie",
            },
            {"ai_response": None, "fallback": "Brak odpowiedzi z pluginu"},
            {},  # Missing ai_response field
        ]

        for scenario in fallback_scenarios:
            response_text = scenario.get("ai_response")

            if not response_text:
                # Fallback should be triggered
                fallback_text = scenario.get("fallback", "WystƒÖpi≈Ç b≈ÇƒÖd")
                final_response = fallback_text
            else:
                final_response = response_text

            assert len(final_response) > 0
            logger.info(f"Fallback handling works: {final_response}")

    @pytest.mark.asyncio
    async def test_long_response_handling_over_200_chars(self, client_helper):
        """Test: Obs≈Çuga d≈Çugich odpowiedzi (>200 znak√≥w)"""
        long_response = "A" * 300  # 300 character response

        mock_server_data = {
            "ai_response": long_response,
            "intent": "long_response_test",
        }

        received_text = mock_server_data["ai_response"]

        assert len(received_text) > 200
        assert len(received_text) == 300
        assert received_text == long_response
        logger.info(f"Long response handled correctly: {len(received_text)} characters")

    @pytest.mark.asyncio
    async def test_format_error_handling_missing_text_field(self, client_helper):
        """Test: Obs≈Çuga b≈Çƒôd√≥w formatu (np. brak pola text)"""
        malformed_responses = [
            {},  # Empty response
            {"intent": "test"},  # Missing ai_response
            {"ai_response": None},  # Null response
            {"error": "Server error"},  # Error response
        ]

        for malformed_response in malformed_responses:
            response_text = malformed_response.get("ai_response")

            if not response_text:
                # Should handle missing/invalid response gracefully
                error_message = malformed_response.get(
                    "error", "Nieprawid≈Çowa odpowied≈∫ serwera"
                )
                final_text = error_message
            else:
                final_text = response_text

            assert isinstance(final_text, str)
            assert len(final_text) > 0
            logger.info(f"Format error handled: {final_text}")


# üîä 5. Synteza mowy (TTS) Tests
class TestTextToSpeech:
    """Test text-to-speech functionality."""

    @pytest.mark.asyncio
    async def test_response_read_immediately_after_receiving(self, client_helper):
        """Test: Odpowied≈∫ jest czytana natychmiast po odebraniu"""
        test_text = "This should be read immediately"

        with patch("asyncio.create_task") as mock_create_task:
            # Mock TTS task creation
            mock_task = AsyncMock()
            mock_create_task.return_value = mock_task

            # Simulate immediate TTS start
            start_time = time.time()
            tts_task = asyncio.create_task(asyncio.sleep(0.1))  # Mock TTS
            await asyncio.sleep(0.05)  # Small delay to check timing

            elapsed_time = time.time() - start_time

            assert elapsed_time < 0.2  # Should start quickly
            assert mock_create_task.called
            logger.info(f"TTS started immediately: {elapsed_time:.3f}s delay")

    @pytest.mark.asyncio
    async def test_streaming_works_not_waiting_for_full_text(self, client_helper):
        """Test: Streamowanie dzia≈Ça (nie czeka na ca≈Çy tekst)"""
        text_chunks = ["Hello ", "this is ", "streaming ", "text-to-speech"]

        spoken_chunks = []

        async def mock_speak_chunk(chunk):
            spoken_chunks.append(chunk)
            await asyncio.sleep(0.1)  # Simulate speaking time

        # Simulate streaming TTS
        tasks = []
        for chunk in text_chunks:
            task = asyncio.create_task(mock_speak_chunk(chunk))
            tasks.append(task)

        await asyncio.gather(*tasks)

        assert len(spoken_chunks) == len(text_chunks)
        assert spoken_chunks == text_chunks
        logger.info(f"Streaming TTS works: {len(spoken_chunks)} chunks processed")

    @pytest.mark.asyncio
    async def test_no_temporary_file_errors(self, client_helper):
        """Test: Brak b≈Çƒôd√≥w z plikami tymczasowymi"""
        with patch("tempfile.NamedTemporaryFile") as mock_temp_file:
            mock_file = MagicMock()
            mock_file.name = "test_tts_temp.wav"
            mock_temp_file.return_value.__enter__.return_value = mock_file

            # Simulate TTS file creation and cleanup
            with tempfile.NamedTemporaryFile(suffix=".wav") as temp_audio:
                temp_path = temp_audio.name

                # File should be created successfully
                assert temp_path.endswith(".wav")

                # Simulate writing audio data
                temp_audio.write(b"fake audio data")
                temp_audio.flush()

                # File should exist during use
                assert Path(temp_path).exists()

            # File should be cleaned up after context exit
            # In real scenario, temp file would be deleted automatically
            logger.info("Temporary file handling works correctly")

    @pytest.mark.asyncio
    async def test_speech_interruption_and_cancellation(self, client_helper):
        """Test: Mo≈ºliwo≈õƒá przerwania lub anulowania wypowiedzi"""
        with patch("asyncio.create_task") as mock_create_task:
            mock_task = AsyncMock()
            mock_task.cancel = MagicMock()
            mock_create_task.return_value = mock_task

            # Start TTS task
            tts_task = asyncio.create_task(asyncio.sleep(10))  # Long-running TTS

            # Simulate interruption after short time
            await asyncio.sleep(0.1)
            tts_task.cancel()

            # Task should be cancelled
            assert tts_task.cancelled()
            logger.info("TTS interruption/cancellation works")

    @pytest.mark.asyncio
    async def test_tts_error_fallback_to_text(self, client_helper):
        """Test: Obs≈Çuga b≈Çƒôdu TTS (np. API pad≈Ço ‚Üí fallback na tekst)"""
        test_text = "This text should be displayed if TTS fails"

        with patch(
            "openai.audio.speech.create", side_effect=Exception("TTS API failed")
        ):
            try:
                # Simulate TTS API call
                import openai

                openai.audio.speech.create(
                    model="tts-1", voice="alloy", input=test_text
                )
                assert False, "Expected TTS error but none occurred"

            except Exception as e:
                # TTS failed, fall back to text display
                fallback_text = test_text

                assert "TTS API failed" in str(e)
                assert fallback_text == test_text
                logger.info(f"TTS fallback to text works: {fallback_text}")


# üß© 6. Overlay (rollbackowa wersja) Tests
class TestOverlay:
    """Test overlay functionality."""

    @pytest.mark.asyncio
    async def test_displays_user_text_and_response(self, client_helper):
        """Test: Wy≈õwietla tekst u≈ºytkownika i odpowied≈∫"""
        user_text = "User query test"
        ai_response = "AI response test"

        overlay_data = {
            "user_input": user_text,
            "ai_response": ai_response,
            "timestamp": time.time(),
        }

        # Simulate overlay display
        displayed_user_text = overlay_data["user_input"]
        displayed_ai_response = overlay_data["ai_response"]

        assert displayed_user_text == user_text
        assert displayed_ai_response == ai_response
        assert len(displayed_user_text) > 0
        assert len(displayed_ai_response) > 0
        logger.info("Overlay displays both texts correctly")

    @pytest.mark.asyncio
    async def test_no_crash_on_empty_response(self, client_helper):
        """Test: Nie crashuje przy pustej odpowiedzi"""
        empty_responses = ["", None, {}, []]

        for empty_response in empty_responses:
            overlay_data = {
                "user_input": "Test query",
                "ai_response": empty_response or "Brak odpowiedzi",
            }

            # Overlay should handle empty responses gracefully
            display_text = overlay_data["ai_response"]

            assert isinstance(display_text, str)
            assert len(display_text) > 0
            logger.info(
                f"Empty response handled: {repr(empty_response)} -> {display_text}"
            )

    @pytest.mark.asyncio
    async def test_scales_on_different_resolutions(self, client_helper):
        """Test: Skaluje siƒô na r√≥≈ºnych rozdzielczo≈õciach"""
        resolutions = [
            (1920, 1080),  # Full HD
            (1366, 768),  # HD
            (3840, 2160),  # 4K
            (1280, 720),  # HD Ready
        ]

        for width, height in resolutions:
            # Simulate overlay scaling
            scale_factor = min(width / 1920, height / 1080)  # Scale relative to Full HD

            overlay_config = {
                "width": int(400 * scale_factor),
                "height": int(200 * scale_factor),
                "font_size": int(14 * scale_factor),
            }

            assert overlay_config["width"] > 0
            assert overlay_config["height"] > 0
            assert overlay_config["font_size"] > 0
            logger.info(f"Overlay scales for {width}x{height}: {overlay_config}")

    @pytest.mark.asyncio
    async def test_readability_and_contrast_day_night(self, client_helper):
        """Test: Czytelno≈õƒá i kontrast ‚Äì test noc/dzie≈Ñ"""
        themes = [
            {"name": "day", "bg_color": "#FFFFFF", "text_color": "#000000"},
            {"name": "night", "bg_color": "#000000", "text_color": "#FFFFFF"},
            {"name": "auto", "bg_color": "#333333", "text_color": "#FFFFFF"},
        ]

        for theme in themes:
            # Calculate contrast ratio (simplified)
            bg_brightness = 1.0 if theme["bg_color"] == "#FFFFFF" else 0.0
            text_brightness = 1.0 if theme["text_color"] == "#FFFFFF" else 0.0
            contrast_ratio = abs(bg_brightness - text_brightness)

            # Good contrast should be > 0.5
            assert contrast_ratio >= 0.5, f"Poor contrast in {theme['name']} theme"
            logger.info(f"{theme['name']} theme has good contrast: {contrast_ratio}")

    @pytest.mark.asyncio
    async def test_responsiveness_no_blocking(self, client_helper):
        """Test: Responsywno≈õƒá ‚Äì nie blokuje innych element√≥w"""
        with patch("asyncio.create_task") as mock_create_task:
            # Mock overlay update as async task
            overlay_task = AsyncMock()
            mock_create_task.return_value = overlay_task

            # Simulate non-blocking overlay update
            start_time = time.time()
            asyncio.create_task(asyncio.sleep(0.01))  # Mock overlay update

            # Other operations should not be blocked
            await asyncio.sleep(0.01)
            other_operation_time = time.time() - start_time

            assert other_operation_time < 0.1  # Should be quick, non-blocking
            assert mock_create_task.called
            logger.info(f"Overlay is non-blocking: {other_operation_time:.3f}s")


# üë§ 7. Sesja u≈ºytkownika Tests
class TestUserSession:
    """Test user session functionality."""

    @pytest.mark.asyncio
    async def test_session_id_generation_and_maintenance(self, client_helper):
        """Test: ID sesji generowane i utrzymywane"""
        import uuid

        # Generate session ID
        session_id = str(uuid.uuid4())

        # Session should be valid UUID
        assert len(session_id) == 36  # UUID format
        assert session_id.count("-") == 4  # UUID has 4 hyphens

        # Session should persist
        stored_session = session_id
        assert stored_session == session_id
        logger.info(f"Session ID generated and maintained: {session_id}")

    @pytest.mark.asyncio
    async def test_multiple_client_instances_different_users(self, client_helper):
        """Test: Obs≈Çuga wielu instancji klienta (r√≥≈ºni u≈ºytkownicy)"""
        users = [
            {"user_id": "user_1", "session_id": "session_1"},
            {"user_id": "user_2", "session_id": "session_2"},
            {"user_id": "user_3", "session_id": "session_3"},
        ]

        for user in users:
            # Each user should have unique session
            assert user["user_id"] != user["session_id"]
            assert len(user["user_id"]) > 0
            assert len(user["session_id"]) > 0

        # All user IDs should be unique
        user_ids = [user["user_id"] for user in users]
        assert len(set(user_ids)) == len(user_ids)

        # All session IDs should be unique
        session_ids = [user["session_id"] for user in users]
        assert len(set(session_ids)) == len(session_ids)

        logger.info(f"Multiple client instances handled: {len(users)} users")

    @pytest.mark.asyncio
    async def test_user_change_affects_history_context(self, client_helper):
        """Test: Zmiana u≈ºytkownika powoduje zmianƒô historii / kontekstu"""
        user1_context = {
            "user_id": "user_1",
            "history": ["Query 1", "Query 2"],
            "preferences": {"language": "pl"},
        }

        user2_context = {
            "user_id": "user_2",
            "history": ["Different query"],
            "preferences": {"language": "en"},
        }

        # Switch between users
        current_context = user1_context
        assert current_context["user_id"] == "user_1"
        assert len(current_context["history"]) == 2

        # Switch to user 2
        current_context = user2_context
        assert current_context["user_id"] == "user_2"
        assert len(current_context["history"]) == 1
        assert current_context["preferences"]["language"] == "en"

        logger.info("User switching changes context correctly")


# üíæ 8. Pamiƒôƒá klienta (je≈õli ma cache/context) Tests
class TestClientMemory:
    """Test client memory/cache functionality."""

    @pytest.mark.asyncio
    async def test_temporary_data_storage_short_term(self, client_helper):
        """Test: Poprawnie przechowuje dane tymczasowe (short term)"""
        temp_data = {
            "recent_queries": ["Query 1", "Query 2", "Query 3"],
            "user_preferences": {"theme": "dark", "voice": "alloy"},
            "session_data": {"start_time": time.time()},
        }

        # Simulate storing temporary data
        stored_data = temp_data.copy()

        assert stored_data["recent_queries"] == temp_data["recent_queries"]
        assert stored_data["user_preferences"] == temp_data["user_preferences"]
        assert "session_data" in stored_data
        logger.info(f"Temporary data stored correctly: {len(stored_data)} items")

    @pytest.mark.asyncio
    async def test_context_reset_functionality(self, client_helper):
        """Test: Reset kontekstu dzia≈Ça"""
        # Initial context with data
        context = {
            "history": ["Query 1", "Query 2"],
            "temp_data": {"key": "value"},
            "cache": {"cached_item": "data"},
        }

        assert len(context["history"]) > 0
        assert "temp_data" in context

        # Reset context
        context = {"history": [], "temp_data": {}, "cache": {}}

        assert len(context["history"]) == 0
        assert len(context["temp_data"]) == 0
        assert len(context["cache"]) == 0
        logger.info("Context reset works correctly")

    @pytest.mark.asyncio
    async def test_no_memory_leak_100_interactions(self, client_helper):
        """Test: Nie przecieka pamiƒôƒá ‚Äì test 100 interakcji"""
        memory_usage = []

        for i in range(100):
            # Simulate interaction
            interaction_data = {
                "id": i,
                "query": f"Test query {i}",
                "response": f"Test response {i}",
                "timestamp": time.time(),
            }

            # In real scenario, this would be stored in memory
            # For testing, we simulate memory usage tracking
            memory_usage.append(len(str(interaction_data)))

            # Clean up old interactions (keep only last 10)
            if len(memory_usage) > 10:
                memory_usage.pop(0)

        # Memory usage should be bounded
        assert len(memory_usage) <= 10
        logger.info(
            f"Memory usage controlled after 100 interactions: {len(memory_usage)} items"
        )


# ‚ö†Ô∏è 9. Fallbacki i edge case'y Tests
class TestFallbacksAndEdgeCases:
    """Test fallbacks and edge cases."""

    @pytest.mark.asyncio
    async def test_no_server_response_shows_message(self, client_helper):
        """Test: Brak odpowiedzi z serwera ‚Üí pokazanie komunikatu"""
        with patch(
            "aiohttp.ClientSession.post",
            side_effect=ConnectionError("Server not available"),
        ):
            try:
                import aiohttp

                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "http://localhost:8001/api/ai_query", json={}
                    ) as response:
                        await response.json()

                assert False, "Expected connection error"

            except ConnectionError:
                # Should show user-friendly message
                error_message = "Serwer nie odpowiada. Sprawd≈∫ po≈ÇƒÖczenie internetowe."

                assert len(error_message) > 0
                assert "serwer" in error_message.lower()
                logger.info(f"Server unavailable message: {error_message}")

    @pytest.mark.asyncio
    async def test_tts_error_shows_text(self, client_helper):
        """Test: B≈ÇƒÖd TTS ‚Üí wy≈õwietlenie tekstu"""
        test_text = "This text should be displayed when TTS fails"

        with patch("openai.audio.speech.create", side_effect=Exception("TTS failed")):
            try:
                # TTS attempt fails
                raise Exception("TTS failed")
            except Exception:
                # Fall back to text display
                display_text = test_text

                assert display_text == test_text
                assert len(display_text) > 0
                logger.info(f"TTS fallback to text: {display_text}")

    @pytest.mark.asyncio
    async def test_transcription_error_not_understood_message(self, client_helper):
        """Test: B≈ÇƒÖd transkrypcji ‚Üí info ‚Äûnie zrozumia≈Çem" """
        with patch("whisper.load_model") as mock_load_model:
            mock_model = MagicMock()
            mock_model.transcribe.side_effect = Exception("Transcription failed")
            mock_load_model.return_value = mock_model

            try:
                # Transcription attempt fails
                mock_model.transcribe("audio_file.wav")
                assert False, "Expected transcription error"

            except Exception:
                # Show "not understood" message
                error_message = "Nie zrozumia≈Çem. Spr√≥buj ponownie."

                assert len(error_message) > 0
                assert "nie zrozumia≈Çem" in error_message.lower()
                logger.info(f"Transcription error message: {error_message}")

    @pytest.mark.asyncio
    async def test_network_error_retry_or_info(self, client_helper):
        """Test: B≈ÇƒÖd sieci ‚Üí ponowna pr√≥ba lub informacja dla u≈ºytkownika"""
        retry_count = 0
        max_retries = 3

        async def attempt_connection():
            nonlocal retry_count
            retry_count += 1

            if retry_count < max_retries:
                raise ConnectionError("Network error")
            else:
                return {"status": "success", "retries": retry_count}

        # Simulate retry logic
        try:
            result = await attempt_connection()
            assert False, "Should fail on first attempts"
        except ConnectionError:
            try:
                result = await attempt_connection()
                assert False, "Should fail on second attempt"
            except ConnectionError:
                result = await attempt_connection()

                assert result["status"] == "success"
                assert result["retries"] == 3
                logger.info(f"Network retry works: {result['retries']} attempts")


# üß™ 10. Scenariusze testowe (kombinacje) Tests
class TestTestScenarios:
    """Test various combined scenarios."""

    @pytest.mark.asyncio
    async def test_short_questions(self, client_helper):
        """Test: Kr√≥tkie pytania (np. ‚Äûkt√≥ra godzina?")"""
        short_questions = [
            "Kt√≥ra godzina?",
            "Pogoda?",
            "Co s≈Çychaƒá?",
            "Gdzie jestem?",
            "Kiedy?",
        ]

        for question in short_questions:
            assert len(question) < 20  # Short question
            assert question.endswith("?")  # Proper question format

            # Mock processing short question
            processed = question.strip().lower()
            assert len(processed) > 0
            logger.info(f"Short question processed: {question}")

    @pytest.mark.asyncio
    async def test_long_questions(self, client_helper):
        """Test: D≈Çugie pytania (‚Äûczy mo≈ºesz podsumowaƒá mi dzie≈Ñ?")"""
        long_questions = [
            "Czy mo≈ºesz podsumowaƒá mi dzie≈Ñ i powiedzieƒá co najwa≈ºniejszego siƒô wydarzy≈Ço?",
            "Jakie sƒÖ najlepsze sposoby na zwiƒôkszenie produktywno≈õci podczas pracy zdalnej w domu?",
            "Czy m√≥g≈Çby≈õ mi wyt≈Çumaczyƒá jak dzia≈Ça sztuczna inteligencja w prostych s≈Çowach?",
        ]

        for question in long_questions:
            assert len(question) > 50  # Long question

            # Mock processing long question
            words = question.split()
            assert len(words) > 10  # Many words
            logger.info(f"Long question processed: {len(words)} words")

    @pytest.mark.asyncio
    async def test_unusual_questions(self, client_helper):
        """Test: Pytania nietypowe (‚Äûczy znasz Ricka?")"""
        unusual_questions = [
            "Czy znasz Ricka?",
            "Co my≈õlisz o kolarstwie na Marsie?",
            "Ile wa≈ºy niebiesko≈õƒá?",
            "Czy mo≈ºna ugotowaƒá d≈∫wiƒôk?",
        ]

        for question in unusual_questions:
            # Unusual questions should still be processed
            assert len(question) > 0

            # Mock AI response for unusual question
            ai_response = "To nietypowe pytanie. Czy mo≈ºesz je doprecyzowaƒá?"

            assert len(ai_response) > 0
            logger.info(f"Unusual question handled: {question}")

    @pytest.mark.asyncio
    async def test_interrupting_tts_with_new_question(self, client_helper):
        """Test: Przerywanie TTS kolejnym pytaniem"""
        with patch("asyncio.create_task") as mock_create_task:
            # Mock TTS task
            tts_task = AsyncMock()
            tts_task.cancel = MagicMock()
            mock_create_task.return_value = tts_task

            # Start TTS
            current_tts = asyncio.create_task(asyncio.sleep(5))  # Long TTS

            # Interrupt with new question
            await asyncio.sleep(0.1)
            current_tts.cancel()

            # Start new TTS
            new_tts = asyncio.create_task(asyncio.sleep(1))

            assert current_tts.cancelled()
            assert not new_tts.cancelled()
            logger.info("TTS interruption with new question works")

    @pytest.mark.asyncio
    async def test_three_users_simultaneously(self, client_helper):
        """Test: 3 u≈ºytkownik√≥w w tym samym czasie"""
        users = [
            {"id": "user_1", "query": "Query from user 1"},
            {"id": "user_2", "query": "Query from user 2"},
            {"id": "user_3", "query": "Query from user 3"},
        ]

        async def process_user_query(user):
            # Simulate processing user query
            await asyncio.sleep(0.1)
            return f"Response for {user['id']}: {user['query']}"

        # Process all users concurrently
        tasks = [process_user_query(user) for user in users]
        results = await asyncio.gather(*tasks)

        assert len(results) == 3
        for i, result in enumerate(results):
            assert users[i]["id"] in result
            assert users[i]["query"] in result

        logger.info(f"3 users processed simultaneously: {len(results)} responses")

    @pytest.mark.asyncio
    async def test_client_runs_over_1_hour_no_crash(self, client_helper):
        """Test: klient dzia≈Ça przez >1h i nie crashuje (symulowane)"""
        # Simulate long-running client (accelerated test)
        start_time = time.time()
        iterations = 100  # Simulate 1 hour worth of interactions

        for i in range(iterations):
            # Simulate typical client operation
            await asyncio.sleep(0.01)  # Small delay to simulate real operations

            # Mock memory cleanup every 10 iterations
            if i % 10 == 0:
                # Simulate garbage collection / cleanup
                pass

        elapsed_time = time.time() - start_time

        # Should complete without crashing
        assert elapsed_time < 10  # Test should be fast
        logger.info(
            f"Long-running client simulation: {iterations} iterations in {elapsed_time:.2f}s"
        )


# üß∞ Narzƒôdzia pomocnicze do test√≥w
class TestHelpersAndTools:
    """Test helper tools for client testing."""

    @pytest.mark.asyncio
    async def test_log_viewer_client_send_receive(self, client_helper):
        """Test: log_viewer: do podglƒÖdu, czy klient wysy≈Ça/odbiera poprawnie"""
        log_entries = []

        # Mock log viewer functionality
        def log_client_activity(action, data):
            log_entry = {"timestamp": time.time(), "action": action, "data": data}
            log_entries.append(log_entry)

        # Simulate client activities
        log_client_activity("send", {"query": "Test query", "user_id": "test_user"})
        log_client_activity(
            "receive", {"response": "Test response", "status": "success"}
        )

        assert len(log_entries) == 2
        assert log_entries[0]["action"] == "send"
        assert log_entries[1]["action"] == "receive"
        logger.info(f"Log viewer tracks activities: {len(log_entries)} entries")

    @pytest.mark.asyncio
    async def test_dev_mode_debug_raw_responses(self, client_helper):
        """Test: dev_mode: opcja debugowania z surowymi odpowiedziami"""
        config = client_helper.create_test_config(dev_mode=True)

        raw_response = {
            "ai_response": "Test response",
            "debug_info": {
                "processing_time": 0.5,
                "model_used": "gpt-4-nano",
                "intent_confidence": 0.95,
            },
        }

        if config.get("dev_mode"):
            # In dev mode, show debug info
            debug_output = raw_response.get("debug_info", {})

            assert "processing_time" in debug_output
            assert "model_used" in debug_output
            assert "intent_confidence" in debug_output
            logger.info(f"Dev mode debug info: {debug_output}")

    @pytest.mark.asyncio
    async def test_test_user_json_different_profiles(self, client_helper):
        """Test: test_user.json: symulacja r√≥≈ºnych profili u≈ºytkownik√≥w"""
        test_profiles = [
            {
                "user_id": "test_user_basic",
                "preferences": {"language": "pl", "voice": "alloy"},
                "subscription": "free",
            },
            {
                "user_id": "test_user_premium",
                "preferences": {"language": "en", "voice": "nova"},
                "subscription": "premium",
            },
            {
                "user_id": "test_user_multilingual",
                "preferences": {"language": "auto", "voice": "echo"},
                "subscription": "free",
            },
        ]

        for profile in test_profiles:
            assert "user_id" in profile
            assert "preferences" in profile
            assert "subscription" in profile

            # Each profile should have different settings
            assert len(profile["user_id"]) > 0
            assert profile["subscription"] in ["free", "premium"]

        logger.info(f"Test user profiles work: {len(test_profiles)} profiles")

    @pytest.mark.asyncio
    async def test_network_monitor_network_error_tests(self, client_helper):
        """Test: network_monitor: do test√≥w b≈Çƒôd√≥w sieci"""
        network_scenarios = [
            {"type": "timeout", "delay": 5.0},
            {"type": "connection_error", "error": "Connection refused"},
            {"type": "dns_error", "error": "DNS resolution failed"},
            {"type": "slow_connection", "delay": 3.0},
        ]

        for scenario in network_scenarios:
            # Simulate network monitoring
            if scenario["type"] == "timeout":
                # Simulate timeout detection
                assert scenario["delay"] > 2.0
                logger.info(f"Network monitor detects timeout: {scenario['delay']}s")

            elif scenario["type"] == "connection_error":
                # Simulate connection error detection
                assert "error" in scenario
                logger.info(f"Network monitor detects error: {scenario['error']}")

            elif scenario["type"] == "slow_connection":
                # Simulate slow connection detection
                assert scenario["delay"] > 1.0
                logger.info(
                    f"Network monitor detects slow connection: {scenario['delay']}s"
                )


# Performance and integration markers
pytestmark = [
    pytest.mark.asyncio,
    pytest.mark.client_tests,
]

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
